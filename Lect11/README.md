## DL4ES, Лекция 11

#### Роль начального приближения в оптимизации глубоких нейросетей. Варианты инициализации.



Лекция продолжает тему начального приближения значений параметров (весов) в оптимизации глубоких нейросетей и влиянию различных методов инициализации на процесс обучения.

Обсуждается, как неправильная инициализация может привести к замедлению или даже невозможности обучения из-за проблем с распределением активаций по слоям.

Рассматриваются различные подходы к инициализации, включая методы, предложенные в работах [Xavier Glorot (2010)](https://proceedings.mlr.press/v9/glorot10a.html) и [Kaiming He (2015)](https://arxiv.org/abs/1502.01852), которые адаптированы под конкретные функции активации (тангенс гиперболический и ReLU соответственно).

Подчеркивается, что выбор метода инициализации зависит от типа функции активации и может значительно влиять на скорость и качество обучения нейросети.

В заключение анонсируется обсуждение пакетной нормализации как способа поддержания стабильного распределения активаций в процессе обучения.


