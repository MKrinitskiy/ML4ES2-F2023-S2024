## DL4ES, Лекция 12

#### Инициализация нейросети; пакетная нормализация (batch normalization); искусственное дополнение данных (data augmentation).



Лекция охватывает две ключевые темы: инициализацию весов нейросетей и пакетную нормализацию (batch normalization).

Обсуждается важность специальной инициализации весов для эффективного начала обучения нейросети, чтобы функция потерь была информативной и оптимизация могла успешно стартовать.

Рассматривается, как инициализация влияет на распределение активаций внутри сети, и подчеркивается важность поддержания дисперсии активаций на определенном уровне для предотвращения исчезающих или взрывающихся градиентов в глубоких сетях.

Далее вводится понятие пакетной нормализации как метода, позволяющего стабилизировать распределение активаций на протяжении обучения, делая среднее близким к нулю и дисперсию близкой к единице. Это способствует ускорению и стабилизации процесса обучения, а также улучшению обобщающей способности модели.

Объясняется, как пакетная нормализация влияет на градиенты и оптимизацию, и рассматриваются её особенности, включая различное поведение во время обучения и инференса.

В заключение кратко упоминается искусственное дополнение данных (data augmentation) как способ улучшения обобщающей способности модели за счет введения дополнительных, искусственно созданных данных, близких к исходным.
