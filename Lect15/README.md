# ML4ES2, Лекция 15

Функции активаций в искусственных нейронных сетях и их свойства. Sigmoid, tanh, ReLU, Leaky ReLU, PReLU, ELU, SELU, Swish, Mish.



Лекция посвящена обзору различных функций активации в искусственных нейронных сетях, их свойствам, преимуществам и недостаткам. Обсуждение начинается с полносвязных сетей, их обучения, вычисления градиентов и проблем, возникающих при обучении, таких как затухание градиентов и насыщение функций активации. Далее рассмотрены следующие функции активации:

1. **Sigmoid** и гиперболический тангенс **tanh**: функции активации, исторически получившие распространение со времен логистической регрессии. Обладают свойством насыщения на бесконечности в обеих сторонах. Кроме этого, характерны проблемой затухающих градиентов и активаций.
2. **ReLU (Rectified Linear Unit)**: Простая и эффективная функция активации, решающая проблему затухания градиентов, но имеющая проблему "мертвых нейронов".
3. **Leaky ReLU и Parametric ReLU**: Модификации ReLU, направленные на решение проблемы "мертвых нейронов" за счет добавления небольшого наклона к отрицательной части функции.
4. **ELU (Exponential Linear Unit)** и **Scaled ELU**: Функции активации, которые пытаются сохранить среднее значение активаций близким к нулю, улучшая обучение, но требуют специфических условий для инициализации весов.
5. **Swish и Mish**: Более новые функции активации, полученные с использованием методов автоматического поиска. Они обладают свойствами, способствующими более гладкому ландшафту функции потерь и улучшенной обобщающей способности моделей.

В лекции подчеркивается, что выбор функции активации зависит от конкретной задачи и данных, и рекомендуется экспериментировать с различными функциями активации для достижения наилучших результатов. В заключение акцентируется внимание на важности гладкости ландшафта функции потерь, которая должна приводить к повышению обобщающей способности модели, а также на том, что идеальной функции активации, подходящей для всех задач, не существует.

[Функции активации на paperswithcode.com](https://paperswithcode.com/methods/category/activation-functions)

[Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](https://arxiv.org/abs/1511.07289) (ELU)

[Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515) (SELU)

[Searching for Activation Functions](https://arxiv.org/abs/1710.05941) (Swish)

[Mish: A Self Regularized Non-Monotonic Activation Function](https://arxiv.org/abs/1908.08681) (Mish)



Онлайн-графики некоторых функций активации: [link](https://www.desmos.com/calculator/70kt21jplx)